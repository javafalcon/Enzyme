{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('e:/Repoes/jci/')\n",
    "sys.path.append('e:/Repoes/jci/bio/')\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from nlp_transformer import Encoder, create_padding_mask\n",
    "from prepare_seq import protseq_to_vec\n",
    "from tools import displayMetrics, displayMLMetrics, plot_history\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(maxlen, vocab_size, embed_dim, num_heads, ff_dim, \n",
    "               num_blocks, droprate, fl_size, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    \n",
    "    encode_padding_mask = create_padding_mask(inputs)\n",
    "    encoder = Encoder(n_layers=num_blocks, d_model=embed_dim, n_heads=num_heads, \n",
    "                      ffd=ff_dim, input_vocab_size=vocab_size, \n",
    "                      max_seq_len=maxlen, dropout_rate=droprate)\n",
    "    x = encoder(inputs, False, encode_padding_mask)\n",
    "    \n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(droprate)(x)\n",
    "    x = layers.Dense(fl_size, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(droprate)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seqs():\n",
    "    \"\"\"\n",
    "    read Enzyme and not Enzyme sequences, \n",
    "    in which every protein sequence is less than 40% similarity with others.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    seqs:\n",
    "        protein sequences\n",
    "    labels:\n",
    "        if 0 for not Enzyme, else 1 for Enzyme\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # read Enzyme and not Enzyme sequences \n",
    "    seq_records = SeqIO.parse('data/EC_40.fasta', 'fasta')\n",
    "    seq_records = shuffle(list(seq_records), random_state=42)\n",
    "    Enzyme_seqs = []\n",
    "    for seq_record in seq_records:\n",
    "        if len(str(seq_record.seq)) >= 50:\n",
    "            Enzyme_seqs.append(str(seq_record.seq))\n",
    "            \n",
    "    seq_records = SeqIO.parse('data/NotEC_40.fasta', 'fasta')\n",
    "    seq_records = shuffle(list(seq_records), random_state=42)\n",
    "    notEnzyme_seqs = []\n",
    "    for seq_record in seq_records:\n",
    "        if len(str(seq_record.seq)) >= 50:\n",
    "            notEnzyme_seqs.append(str(seq_record.seq))\n",
    "    notEnzyme_seqs = shuffle(notEnzyme_seqs)\n",
    "    notEnzyme_seqs = notEnzyme_seqs[:len(Enzyme_seqs)]\n",
    "    \n",
    "    \n",
    "    seqs = Enzyme_seqs + notEnzyme_seqs\n",
    "    labels = [1 for i in range(len(Enzyme_seqs))] + [0 for i in range(len(notEnzyme_seqs))]\n",
    "\n",
    "    return seqs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_predictor(X_train, y_train, X_test, y_test, modelfile, params):\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    model = buildModel(params['maxlen'], params['vocab_size'], params['embed_dim'], \n",
    "                    params['num_heads'], params['ff_dim'],  params['num_blocks'], \n",
    "                    params['droprate'], params['fl_size'], params['num_classes'])\n",
    "    model.summary()\n",
    "\n",
    "    checkpoint = callbacks.ModelCheckpoint(modelfile, monitor='val_loss',\n",
    "                                       save_best_only=True, \n",
    "                                       save_weights_only=True, \n",
    "                                       verbose=1)\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=params['batch_size'], epochs=params['epochs'], \n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[checkpoint]\n",
    "        )\n",
    "\n",
    "    plot_history(history)\n",
    "\n",
    "    #model.load_weights(modelfile)\n",
    "    score = model.predict(X_test)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer net params\n",
    "params = {}\n",
    "params['vocab_size'] = 24\n",
    "params['maxlen'] = 500\n",
    "params['embed_dim'] = 16 # Embedding size for each token\n",
    "params['num_heads'] = 4  # Number of attention heads\n",
    "params['ff_dim'] = 128  # Hidden layer size in feed forward network inside transformer\n",
    "params['num_blocks'] = 12\n",
    "params['droprate'] = 0.2\n",
    "params['fl_size'] = 96\n",
    "params['num_classes'] = 2\n",
    "params['epochs'] = 20\n",
    "params['batch_size'] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "seqs, labels = load_seqs()\n",
    "\n",
    "# split data into train and test\n",
    "seqs_train, seqs_test, labels_train, labels_test = train_test_split(seqs, labels, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform protein sequence to word vector\n",
    "\n",
    "X_train = protseq_to_vec(seqs_train, padding_position=\"post\", maxlen=params['maxlen'])\n",
    "X_test = protseq_to_vec(seqs_test, padding_position=\"post\", maxlen=params['maxlen'])\n",
    "\n",
    "y_train = keras.utils.to_categorical(labels_train, params['num_classes'])\n",
    "y_test = keras.utils.to_categorical(labels_test, params['num_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Equal (TensorFlo [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Cast (TensorFlow [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1, 1, 500)]       0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, 500, 16)           61824     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 96)                1632      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 2)                 194       \n",
      "=================================================================\n",
      "Total params: 63,650\n",
      "Trainable params: 63,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1233/1233 [==============================] - ETA: 0s - loss: 0.6981 - accuracy: 0.4974\n",
      "Epoch 00001: val_loss improved from inf to 0.69372, saving model to ./model/ec/ec_trainsformer_500_pos.h5\n",
      "1233/1233 [==============================] - 2292s 2s/step - loss: 0.6981 - accuracy: 0.4974 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "1233/1233 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5008\n",
      "Epoch 00002: val_loss improved from 0.69372 to 0.69334, saving model to ./model/ec/ec_trainsformer_500_pos.h5\n",
      "1233/1233 [==============================] - 2294s 2s/step - loss: 0.6938 - accuracy: 0.5008 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "1233/1233 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5009"
     ]
    }
   ],
   "source": [
    "# training and test\n",
    "modelfile = './model/ec/ec_trainsformer_{}_{}.h5'.format(params[\"maxlen\"], \"pos\")\n",
    "score = transformer_predictor(X_train, y_train, X_test, y_test, modelfile, params)\n",
    "pred = np.argmax(score, 1)\n",
    "displayMetrics(np.argmax(y_test, 1), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
