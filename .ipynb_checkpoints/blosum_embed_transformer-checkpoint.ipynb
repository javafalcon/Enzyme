{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入氨基酸blosum62矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "blosum = sio.loadmat('e:/repoes/jci/bio/blosum.mat')\n",
    "blosumMatrix = blosum['blosum62']\n",
    "alphabet = 'ARNDCQEGHILKMFPSTWYVBZX*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoidfun(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 氨基酸编码字典\n",
    "amino_acid_blosum_dict = {}\n",
    "for a in alphabet:\n",
    "    amino_acid_blosum_dict[a] = sigmoidfun(blosumMatrix[alphabet.index(a),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('e:/Repoes/jci/')\n",
    "sys.path.append('e:/Repoes/jci/bio/')\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import displayMetrics, displayMLMetrics, plot_history\n",
    "from prepare_seq import protseq_to_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将padding位mark，原来为0的padding项的mark输出为1\n",
    "def create_padding_mask(seq):\n",
    "    # 获取为0的padding项\n",
    "    seq = tf.cast(tf.math.equal(tf.reduce_sum(seq,axis=-1), 0), tf.float32)\n",
    "    # 扩充维度以便用于attention矩阵\n",
    "    return seq[:, np.newaxis, np.newaxis, :] # (batch_size, 1, 1, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成[batch_size, maxlen, embed_dim]的矩阵掩码\n",
    "def code_and_padding(seqs, padding_position=\"post\", maxlen=1000):\n",
    "    amino_acids = \"ARNDCQEGHILKMFPSTWYVX\"\n",
    "    regexp = re.compile('[^ARNDCQEGHILKMFPSTWYVX]')\n",
    "    X = []\n",
    "    if padding_position == \"post\":\n",
    "        for seq in seqs:\n",
    "            seq = regexp.sub('X', seq)\n",
    "            i = 0\n",
    "            t = []\n",
    "            while i < len(seq) and i < maxlen:\n",
    "                t.append(amino_acid_blosum_dict[seq[i]])\n",
    "                i += 1\n",
    "            while i < maxlen:\n",
    "                t.append(np.zeros((24,)))\n",
    "                i += 1\n",
    "            X.append(t)\n",
    "    elif padding_position == \"pre\":\n",
    "        for seq in seqs:\n",
    "            i = -1\n",
    "            t = []\n",
    "            while abs(i) <= len(seq) and abs(i) <= maxlen:\n",
    "                t.insert(0, amino_acid_blosum_dict[seq[i]])\n",
    "                i -= 1\n",
    "            while abs(i) <= maxlen:\n",
    "                t.insert(0, np.zeros((24,)))\n",
    "                i -= 1\n",
    "            X.append(t)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "位置函数\n",
    "    基于角度的位置编码方法。计算位置编码矢量的长度\n",
    "    Parameters\n",
    "    ----------\n",
    "    pos : \n",
    "        在句子中字的位置序号，取值范围是[0, max_sequence_len).\n",
    "    i   : int\n",
    "        字向量的维度，取值范围是[0, embedding_dim).\n",
    "    embedding_dim : int\n",
    "        字向量最大维度， 即embedding_dim的最大值.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float32\n",
    "        第pos位置上对应矢量的长度.\n",
    "\"\"\"\n",
    "def get_angles(pos, i, embed_dim):\n",
    "    angel_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "    return pos * angel_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "def position_encoding(position, embed_dim):\n",
    "    angel_rads = get_angles(np.arange(position)[:, np.newaxis], \n",
    "                            np.arange(embed_dim)[np.newaxis, :], \n",
    "                            embed_dim)\n",
    "    sines = np.sin(angel_rads[:, 0::2])\n",
    "    cones = np.cos(angel_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cones], axis=-1)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意力机制\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    # query key 相乘获取匹配关系\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    # 使用dk进行缩放\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    # 掩码.将被掩码的token乘以-1e9（表示负无穷），这样\n",
    "    # softmax之后就为0， 不对其它token产生影响\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    # 通过softmax获取attention权重\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    # attention乘上value\n",
    "    output = tf.matmul(attention_weights, v) # (..., seq_len_v, depth)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造mutil head attention层\n",
    "\n",
    "multi-head attention包含3部分：线性层与分头-缩放点积注意力-头链接-末尾线性层\n",
    "每个多头注意块有三个输入：Q（查询），K（密钥），V（值）。它们通过第一层线性层并分成多个头\n",
    "Q，K，V不是一个单独的注意头，而是分成多个头，因为它允许模型共同参与来自不同表征空间的\n",
    "不同信息。在拆分后，每个头部具有降低的维度，总计算成本与具有全维度的单个头部注意力相同\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # embd_dim必须可以被num_heads整除\n",
    "        assert embed_dim % num_heads == 0\n",
    "        # 分头后的维度\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.wq = layers.Dense(embed_dim)\n",
    "        self.wk = layers.Dense(embed_dim)\n",
    "        self.wv = layers.Dense(embed_dim)\n",
    "        self.dense = layers.Dense(embed_dim)\n",
    "        \n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # 分头前的前向网络，获取q, k, v语义\n",
    "        q = self.wq(q)\n",
    "        k = self.wq(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # 分头\n",
    "        q = self.separate_heads(q, batch_size) # [batch_size, num_heads, seq_len_q, projection_dim]\n",
    "        k = self.separate_heads(k, batch_size)\n",
    "        v = self.separate_heads(v, batch_size)\n",
    "        \n",
    "        # 通过缩放点积注意力层\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # 把多头维度后移\n",
    "        scaled_attention = tf.transpose(scaled_attention, [0, 2, 1, 3])\n",
    "        # 合并多头\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embed_dim))\n",
    "        \n",
    "        # 全连接重塑\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造Transformer层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造前向网络\n",
    "def point_wise_feed_forward_network(d_model, diff):\n",
    "    # d_model 即embed_dim\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Dense(diff, activation='relu'),\n",
    "        layers.Dense(d_model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer编码层\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, ffd, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, ffd)\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training, mask):\n",
    "        att_output, _ = self.mha(inputs, inputs, inputs, mask)\n",
    "        att_output = self.dropout1(att_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + att_output)\n",
    "        \n",
    "        # 前向网络\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer编码块\n",
    "一个transformer编码块包含n_layers个transformer层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, ffd,\n",
    "                 input_vocab_size, max_seq_len, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.pos_embedding = position_encoding(max_seq_len, d_model)\n",
    "        self.encoder_layer = [EncoderLayer(d_model, n_heads, ffd, dropout_rate)\n",
    "                              for _ in range(n_layers)]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training, mask):\n",
    "        seq_len = inputs.shape[-1]\n",
    "        word_emb = tf.cast(inputs, tf.float32)\n",
    "        word_emb *= (tf.cast(self.d_model, tf.float32))\n",
    "        emb = word_emb + self.pos_embedding\n",
    "        x = self.dropout(emb, training=training)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.encoder_layer[i](x, training, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立预测模型\n",
    "模型以transformer编码块为上游任务，从Transformer的编码块中提取特征，然后加上全连接层构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(maxlen, vocab_size, embed_dim, num_heads, ff_dim, \n",
    "               num_blocks, droprate, fl_size, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,24))\n",
    "    \n",
    "    encode_padding_mask = create_padding_mask(inputs)\n",
    "    encoder = Encoder(n_layers=num_blocks, d_model=embed_dim, n_heads=num_heads, \n",
    "                      ffd=ff_dim, input_vocab_size=vocab_size, \n",
    "                      max_seq_len=maxlen, dropout_rate=droprate)\n",
    "    x = encoder(inputs, False, encode_padding_mask)\n",
    "    \n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(droprate)(x)\n",
    "    x = layers.Dense(fl_size, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(droprate)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_predictor(X_train, y_train, X_test, y_test, modelfile, params):\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    model = buildModel(params['maxlen'], params['vocab_size'], params['embed_dim'], \n",
    "                    params['num_heads'], params['ff_dim'],  params['num_blocks'], \n",
    "                    params['droprate'], params['fl_size'], params['num_classes'])\n",
    "    model.summary()\n",
    "\n",
    "    checkpoint = callbacks.ModelCheckpoint(modelfile, monitor='val_loss',\n",
    "                                       save_best_only=True, \n",
    "                                       save_weights_only=True, \n",
    "                                       verbose=1)\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=params['batch_size'], epochs=params['epochs'], \n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[checkpoint]\n",
    "        )\n",
    "\n",
    "    plot_history(history)\n",
    "\n",
    "    #model.load_weights(modelfile)\n",
    "    score = model.predict(X_test)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer net params\n",
    "params = {}\n",
    "params['vocab_size'] = 24\n",
    "params['maxlen'] = 800\n",
    "params['embed_dim'] = 24 # Embedding size for each token\n",
    "params['num_heads'] = 8  # Number of attention heads\n",
    "params['ff_dim'] = 128  # Hidden layer size in feed forward network inside transformer\n",
    "params['num_blocks'] = 12\n",
    "params['droprate'] = 0.2\n",
    "params['fl_size'] = 128\n",
    "params['num_classes'] = 2\n",
    "params['epochs'] = 20\n",
    "params['batch_size'] = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seqs():\n",
    "    \"\"\"\n",
    "    read Enzyme and not Enzyme sequences, \n",
    "    in which every protein sequence is less than 40% similarity with others.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    seqs:\n",
    "        protein sequences\n",
    "    labels:\n",
    "        if 0 for not Enzyme, else 1 for Enzyme\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # read Enzyme and not Enzyme sequences \n",
    "    seq_records = SeqIO.parse('data/EC_40.fasta', 'fasta')\n",
    "    seq_records = shuffle(list(seq_records), random_state=42)\n",
    "    Enzyme_seqs = []\n",
    "    for seq_record in seq_records:\n",
    "        if len(str(seq_record.seq)) >= 50:\n",
    "            Enzyme_seqs.append(str(seq_record.seq))\n",
    "            \n",
    "    seq_records = SeqIO.parse('data/NotEC_40.fasta', 'fasta')\n",
    "    seq_records = shuffle(list(seq_records), random_state=42)\n",
    "    notEnzyme_seqs = []\n",
    "    for seq_record in seq_records:\n",
    "        if len(str(seq_record.seq)) >= 50:\n",
    "            notEnzyme_seqs.append(str(seq_record.seq))\n",
    "    notEnzyme_seqs = shuffle(notEnzyme_seqs)\n",
    "    notEnzyme_seqs = notEnzyme_seqs[:len(Enzyme_seqs)]\n",
    "    \n",
    "    \n",
    "    seqs = Enzyme_seqs + notEnzyme_seqs\n",
    "    labels = [1 for i in range(len(Enzyme_seqs))] + [0 for i in range(len(notEnzyme_seqs))]\n",
    "\n",
    "    return seqs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "seqs, labels = load_seqs()\n",
    "# split data into train and test\n",
    "seqs_train, seqs_test, labels_train, labels_test = train_test_split(seqs, labels, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = code_and_padding(seqs_train, maxlen=params['maxlen'])\n",
    "x_test = code_and_padding(seqs_test, maxlen=params['maxlen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(labels_train, params['num_classes'])\n",
    "y_test = keras.utils.to_categorical(labels_test, params['num_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 800, 24)]         0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowO [(None, 800)]             0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Equal (TensorFlo [(None, 800)]             0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Cast (TensorFlow [(None, 800)]             0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1, 1, 800)]       0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, 800, 24)           98304     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 128)               3200      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 101,762\n",
      "Trainable params: 101,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n",
      "mha.shape: (None, None, 24)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,8,800,800] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/encoder/encoder_layer_5/multi_head_attention_5/Softmax (defined at <ipython-input-11-ea5e74f5547f>:14) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_66911]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/encoder/encoder_layer_5/multi_head_attention_5/Softmax:\n model/encoder/encoder_layer_5/multi_head_attention_5/add (defined at <ipython-input-11-ea5e74f5547f>:12)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c32f413db9a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# training and test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodelfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./model/ec/ec_trainsformer_{}_{}.h5'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"maxlen\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pos\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_predictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisplayMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-3a3fb785ad02>\u001b[0m in \u001b[0;36mtransformer_predictor\u001b[1;34m(X_train, y_train, X_test, y_test, modelfile, params)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         )\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DEV\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DEV\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DEV\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DEV\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DEV\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DEV\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DEV\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DEV\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\DEV\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,8,800,800] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/encoder/encoder_layer_5/multi_head_attention_5/Softmax (defined at <ipython-input-11-ea5e74f5547f>:14) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_66911]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/encoder/encoder_layer_5/multi_head_attention_5/Softmax:\n model/encoder/encoder_layer_5/multi_head_attention_5/add (defined at <ipython-input-11-ea5e74f5547f>:12)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# training and test\n",
    "modelfile = './model/ec/ec_trainsformer_{}_{}.h5'.format(params[\"maxlen\"], \"pos\")\n",
    "score = transformer_predictor(x_train, y_train, x_test, y_test, modelfile, params)\n",
    "pred = np.argmax(score, 1)\n",
    "displayMetrics(np.argmax(y_test, 1), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu] *",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "171px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
